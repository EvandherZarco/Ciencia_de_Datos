{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covarianza y Correlación\n",
    "\n",
    "La covarianza y la correlación son herramientas estadísticas fundamentales para analizar relaciones entre variables cuantitativas. Estas medidas permiten evaluar si existe una relación entre dos variables y, en caso afirmativo, describir la naturaleza de dicha relación.\n",
    "\n",
    "## ¿Qué es la Covarianza?\n",
    "\n",
    "La covarianza es una medida que indica el grado de variación conjunta entre dos variables. Es decir, nos ayuda a entender si los valores de dos variables cambian de manera similar. Matemáticamente, la covarianza entre dos variables $X$ e $Y$ se define como:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{n}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $X_i$ y $Y_i$ son los valores de las variables $X$ e $Y$, respectivamente.\n",
    "- $\\overline{X}$ y $\\overline{Y}$ son las medias de las variables $X$ e $Y$.\n",
    "- $n$ es el número total de observaciones.\n",
    "\n",
    "### Interpretación de la Covarianza\n",
    "\n",
    "1. **Covarianza positiva**: Cuando los valores de $X$ e $Y$ tienden a aumentar o disminuir juntos, la covarianza es positiva. Esto indica una relación directa entre las variables.\n",
    "2. **Covarianza negativa**: Cuando $X$ aumenta y $Y$ disminuye (o viceversa), la covarianza es negativa. Esto indica una relación inversa.\n",
    "3. **Covarianza cercana a cero**: Una covarianza cercana a cero sugiere que no hay una relación lineal evidente entre las variables.\n",
    "\n",
    "### Limitaciones de la Covarianza\n",
    "\n",
    "Aunque la covarianza nos da una idea de la dirección de la relación entre dos variables, tiene limitaciones:\n",
    "- **Escala dependiente**: Su magnitud depende de las unidades de las variables, lo que dificulta comparar covarianzas de diferentes conjuntos de datos.\n",
    "- **No estandarizada**: Esto significa que una covarianza grande no necesariamente indica una relación más fuerte; podría deberse simplemente a las unidades de las variables.\n",
    "\n",
    "Por estas razones, se utiliza la correlación, que normaliza la covarianza para que sea más interpretable. La correlación se estudiará en las siguientes secciones.\n",
    "\n",
    "A continuación, calcularemos la covarianza entre dos conjuntos de datos y exploraremos su significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Listas de ventas y presupuesto de marketing\n",
    "ventas = [10, 12, 15, 18, 22, 25, 30, 35, 40, 45]\n",
    "presupuesto_marketing = [500, 550, 600, 650, 700, 750, 800, 850, 900, 950]\n",
    "\n",
    "# Cálculo de la media de ventas y presupuesto\n",
    "media_ventas = sum(ventas) / len(ventas)\n",
    "media_marketing = sum(presupuesto_marketing) / len(presupuesto_marketing)\n",
    "\n",
    "# Cálculo de la covarianza\n",
    "covarianza = sum((v - media_ventas) * (m - media_marketing) \n",
    "                 for v, m in zip(ventas, presupuesto_marketing)) / len(ventas)\n",
    "\n",
    "# Mostramos el resultado de la covarianza\n",
    "print(f\"Covarianza entre Ventas y Presupuesto de Marketing: {covarianza:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones del Cálculo de la Covarianza\n",
    "\n",
    "Si bien la covarianza es una herramienta poderosa para entender la relación entre dos variables, tiene ciertas limitaciones que es importante tener en cuenta:\n",
    "\n",
    "1. **Falta de Escalabilidad**:\n",
    "   - La covarianza no tiene un rango específico de valores. Esto significa que su magnitud depende de las unidades de las variables, lo que dificulta la interpretación directa de su valor.\n",
    "   - Por ejemplo, una covarianza de 200 puede ser significativa en un conjunto de datos y no tener ningún significado en otro conjunto con una escala diferente.\n",
    "\n",
    "2. **Unidades Dependientes**:\n",
    "   - La covarianza está influenciada por las unidades de medida de las variables. Si cambian las unidades (por ejemplo, de metros a centímetros), el valor de la covarianza también cambia.\n",
    "\n",
    "3. **No Indica Fuerza Relativa**:\n",
    "   - Aunque una covarianza positiva indica que las variables tienden a aumentar juntas, no proporciona una medida normalizada que permita comparar relaciones entre diferentes pares de variables.\n",
    "\n",
    "4. **Linealidad Limitada**:\n",
    "   - La covarianza solo mide relaciones lineales. Si las variables tienen una relación no lineal, la covarianza no capturará completamente esta relación.\n",
    "\n",
    "5. **Dificultad en Comparaciones**:\n",
    "   - Debido a su dependencia de la escala y las unidades, la covarianza no es adecuada para comparar relaciones entre diferentes conjuntos de datos.\n",
    "\n",
    "### Alternativa: Correlación\n",
    "Debido a estas limitaciones, a menudo se prefiere el uso de la **correlación**, ya que es una medida normalizada que varía entre -1 y 1, lo que facilita la interpretación y comparación de relaciones entre diferentes conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlación entre Variables\n",
    "\n",
    "La correlación es una medida estadística que describe el grado de relación entre dos variables cuantitativas. Esta relación nos permite saber si los cambios en una variable están asociados con cambios en otra variable. Por ejemplo, podríamos querer saber si el aumento en el estudio está correlacionado con mejores calificaciones.\n",
    "\n",
    "## Relación con la Covarianza\n",
    "\n",
    "La correlación y la covarianza están íntimamente relacionadas. La correlación es esencialmente una versión normalizada de la covarianza. Mientras que la covarianza mide la dirección de la relación entre dos variables, la correlación escala este valor para que sea más interpretable, situándolo en un rango entre -1 y +1. \n",
    "\n",
    "La fórmula que relaciona la correlación ($r$) con la covarianza es:\n",
    "\n",
    "$$ r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "Donde:\n",
    "- $\\text{Cov}(X, Y)$ es la covarianza entre las variables $X$ e $Y$.\n",
    "- $\\sigma_X$ y $\\sigma_Y$ son las desviaciones estándar de $X$ e $Y$ respectivamente.\n",
    "\n",
    "Esta relación implica que:\n",
    "- La correlación elimina el efecto de las unidades de medida de las variables.\n",
    "- Es posible comparar relaciones entre diferentes pares de variables gracias a la escala normalizada.\n",
    "\n",
    "## Correlación de Pearson\n",
    "\n",
    "La correlación de Pearson es un método para calcular la correlación lineal entre dos variables, denotada generalmente como $r$. Este coeficiente varía entre -1 y +1, donde:\n",
    "- **+1 indica una correlación positiva perfecta**: cuando una variable aumenta, la otra también.\n",
    "- **-1 indica una correlación negativa perfecta**: cuando una variable aumenta, la otra disminuye.\n",
    "- **0 indica que no hay correlación**: las variables no muestran una relación lineal.\n",
    "\n",
    "La fórmula para calcular la correlación de Pearson entre dos variables $X$ e $Y$ es:\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline{X})^2} \\sqrt{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}} $$\n",
    "\n",
    "Donde:\n",
    "- $X_i$ e $Y_i$ son los valores de las variables.\n",
    "- $\\overline{X}$ e $\\overline{Y}$ son las medias de las variables $X$ e $Y$ respectivamente.\n",
    "\n",
    "## Importancia y Supuestos\n",
    "\n",
    "Existen otros tipos de correlaciones, como la correlación de Spearman y la correlación de Kendall, que se utilizan en diferentes contextos, especialmente cuando los datos no cumplen con los supuestos necesarios para la correlación de Pearson. Los principales supuestos para usar la correlación de Pearson son:\n",
    "1. **Linealidad**: La relación entre las variables debe ser lineal.\n",
    "2. **Normalidad**: Los datos de ambas variables deben seguir una distribución normal.\n",
    "3. **Homoscedasticidad**: La varianza de una variable en diferentes valores de la otra debe ser similar.\n",
    "\n",
    "Cuando estos supuestos no se cumplen, es recomendable considerar otros métodos de correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Listas de pesos y alturas con correlación positiva\n",
    "pesos = [57.6, 84.6, 71.3, 72.2, 95.8, 83.2, 79.4, 76.7, 93.2, 95.6, 86.2, 82.0, 49.9, 76.5, 73.4, 76.1, 86.3, 42.1, 68.2, 78.2, 69.4, 58.0, 85.4, 78.3, 63.3, 78.8, 84.9, 75.1, 84.6, 75.9, 70.3, 61.1, 74.0, 77.3, 85.5, 73.6, 80.5, 63.4, 65.6, 73.7, 62.3, 84.8, 80.6, 78.3, 79.1, 99.3, 69.4, 48.6, 77.4, 74.4]\n",
    "alturas = [85.2, 99.4, 95.5, 102.4, 112.0, 132.4, 96.0, 95.8, 92.4, 119.5, 90.1, 102.0, 82.1, 96.4, 118.8, 113.2, 123.6, 70.7, 109.0, 109.7, 106.3, 100.9, 100.7, 100.6, 103.0, 78.6, 124.5, 108.1, 108.8, 105.4, 88.9, 89.3, 112.8, 117.1, 136.8, 100.8, 99.8, 89.3, 85.7, 100.8, 97.5, 109.0, 117.4, 102.5, 101.9, 113.7, 82.2, 68.3, 92.4, 115.1]\n",
    "\n",
    "# Cálculo de la media de pesos y alturas\n",
    "media_pesos = sum(pesos) / len(pesos)\n",
    "media_alturas = sum(alturas) / len(alturas)\n",
    "\n",
    "# Cálculo de las sumas de productos de desviaciones\n",
    "suma_productos = sum((p - media_pesos) * (a - media_alturas) for p, a in zip(pesos, alturas))\n",
    "suma_cuadrados_pesos = sum((p - media_pesos)**2 for p in pesos)\n",
    "suma_cuadrados_alturas = sum((a - media_alturas)**2 for a in alturas)\n",
    "\n",
    "# Cálculo del coeficiente de correlación de Pearson\n",
    "r_pearson = suma_productos / (suma_cuadrados_pesos**0.5 * suma_cuadrados_alturas**0.5)\n",
    "\n",
    "# Mostramos el resultado y un gráfico\n",
    "print(f\"Coeficiente de Correlación de Pearson: {r_pearson:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los datos\n",
    "plt.scatter(pesos, alturas)\n",
    "plt.title(\"Diagrama de dispersión de Peso vs. Altura\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Altura (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuciones y su Importancia en la Correlación\n",
    "\n",
    "Una **distribución** en estadística describe cómo se dispersan o distribuyen los valores de una variable. Hay muchos tipos de distribuciones, cada una con características únicas que pueden o no ajustarse a diferentes conjuntos de datos.\n",
    "\n",
    "En el estudio de la correlación, es especialmente importante considerar la **distribución normal**, también conocida como distribución gaussiana. Esta distribución es fundamental porque muchos tests estadísticos y métodos, incluyendo la correlación de Pearson, asumen que los datos siguen este tipo de distribución. La distribución normal se caracteriza por:\n",
    "- Ser simétrica alrededor de la media.\n",
    "- Tener la mayoría de los datos concentrados cerca de la media, disminuyendo en frecuencia a medida que se alejan de ella.\n",
    "\n",
    "## Verificando la Normalidad\n",
    "\n",
    "Para verificar si nuestros datos de peso y altura siguen una distribución normal, podemos utilizar un histograma, que nos permite visualizar la forma general de la distribución de los datos. Observamos si la forma del histograma se asemeja a una campana, lo que indicaría una aproximación a la normalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los histogramas\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histograma de pesos\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(pesos, bins=10, color='blue', alpha=0.7)\n",
    "plt.title(\"Histograma de Pesos\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "# Histograma de alturas\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(alturas, bins=10, color='green', alpha=0.7)\n",
    "plt.title(\"Histograma de Alturas\")\n",
    "plt.xlabel(\"Altura (cm)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "plt.tight_layout() # Ajuste automático sobre el área disponible\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homoscedasticidad y Varianza Similar\n",
    "\n",
    "La **homoscedasticidad** es otro supuesto importante cuando se analiza la correlación, especialmente para la correlación de Pearson. Este término se refiere a que las varianzas de las variables estudiadas deben ser similares a lo largo de todos los valores posibles de otras variables.\n",
    "\n",
    "\n",
    "## Calculando Varianza para Verificar Homoscedasticidad\n",
    "\n",
    "Aunque en estadísticas avanzadas se utilizan pruebas específicas para verificar la homoscedasticidad, en una etapa introductoria, podemos comenzar simplemente calculando y comparando las varianzas de las variables de interés. Si las varianzas no son radicalmente diferentes, esto podría ser una primera indicación de que los datos podrían ser homoscedásticos.\n",
    "\n",
    "A continuación, calcularemos las varianzas de nuestras variables de peso y altura para ver cómo se comparan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "# Calcular varianza\n",
    "varianza_pesos = statistics.variance(pesos)\n",
    "varianza_alturas = statistics.variance(alturas)\n",
    "\n",
    "print(f\"Varianza de pesos: {varianza_pesos}\")\n",
    "print(f\"Varianza de alturas: {varianza_alturas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Datos con Distribución Normal Usando NumPy\n",
    "\n",
    "NumPy es una biblioteca de Python ampliamente utilizada en el análisis de datos y la ciencia de datos debido a su eficiencia y versatilidad en el manejo de operaciones matemáticas y estadísticas.\n",
    "\n",
    "## Generación de Datos Normalmente Distribuidos en NumPy\n",
    "\n",
    "NumPy permite generar muestras de datos que siguen una distribución normal mediante la función `numpy.random.normal()`. Los parámetros clave de esta función son:\n",
    "- `loc`: La media de la distribución.\n",
    "- `scale`: La desviación estándar de la distribución.\n",
    "- `size`: El número de muestras a generar.\n",
    "\n",
    "Al usar esta función, podemos simular datos que modelen comportamientos reales en áreas como la investigación científica, la economía, entre otros.\n",
    "\n",
    "Vamos a generar una nueva serie de datos de peso, utilizando NumPy para simular 500 muestras de peso distribuidas normalmente, y redondearemos estos valores a un decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Configurar la semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar 50 muestras de peso con distribución normal y redondear a un decimal\n",
    "pesos_nuevos = np.round(np.random.normal(loc=75, scale=12, size=500), 1)\n",
    "\n",
    "print(\"Pesos generados y redondeados a un decimal:\", pesos_nuevos[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(pesos_nuevos, bins=10, color='blue')\n",
    "plt.title(\"Histograma de Pesos\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout() # Ajuste automático sobre el área disponible\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la Correlación entre Peso y Altura\n",
    "\n",
    "Ahora que hemos generado datos normalmente distribuidos para el peso, vamos a crear un conjunto de datos de altura que tenga una correlación alta con el peso. Esto nos permitirá observar cómo se ve una correlación fuerte en un diagrama de dispersión.\n",
    "\n",
    "## Generación de Alturas con Alta Correlación\n",
    "\n",
    "Para simular una correlación realista y alta entre el peso y la altura, manipularemos los datos de peso para generar alturas correspondientes. Usaremos la siguiente técnica:\n",
    "\n",
    "1. **Correlación Objetivo**: Nos proponemos alcanzar una correlación de aproximadamente 0.9.\n",
    "2. **Generación de Ruido**: Añadimos ruido aleatorio con baja varianza a la relación lineal para simular pequeñas variaciones naturales, manteniendo la correlación alta.\n",
    "3. **Redondeo de Datos**: Redondeamos los datos al décimo más cercano para simular la precisión de las mediciones reales.\n",
    "\n",
    "Esta técnica nos ayudará a entender visualmente el impacto de una correlación alta en análisis estadísticos y cómo podríamos identificarla en estudios reales.\n",
    "\n",
    "Vamos a realizar los cálculo y a graficar estos datos para visualizar la relación entre peso y altura para posteriormente, verificar la correlación obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar alturas hipotéticas con una correlación de aproximadamente 0.9 con los pesos\n",
    "# Añadir ruido aleatorio con baja varianza para mantener la correlación alta\n",
    "ruido = np.random.normal(loc=0, scale=5, size=500)\n",
    "alturas_nuevas = np.round(0.9 * pesos_nuevos + 50 + ruido, 1)\n",
    "\n",
    "print(\"Pesos generados y redondeados a un decimal:\", pesos_nuevos[:10])\n",
    "print(\"Alturas generadas y redondeadas a un decimal:\", alturas_nuevas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo del Coeficiente de Correlación con `np.corrcoef`\n",
    "\n",
    "Para calcular la correlación entre los conjuntos de datos de peso y altura, utilizamos la función `np.corrcoef`, que devuelve una matriz de coeficientes de correlación. Esta matriz es simétrica y tiene un tamaño de 2x2 cuando se comparan dos series de datos:\n",
    "\n",
    "- **Índices [0, 1] y [1, 0]**: Ambos índices acceden al coeficiente de correlación entre las primeras y segundas series de datos ingresadas, en nuestro caso, peso y altura respectivamente. El resultado en ambos índices es el mismo, ya que la correlación entre A y B es igual a la correlación entre B y A.\n",
    "- **Índices [0, 0] y [1, 1]**: Estos índices devuelven la correlación de cada serie de datos consigo misma, que siempre será 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_correlacion=np.corrcoef(pesos_nuevos, alturas_nuevas)\n",
    "print(matriz_correlacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlación entre pesos y alturas:\", matriz_correlacion[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de la Correlación\n",
    "\n",
    "La correlación de aproximadamente 0.9 entre peso y altura es significativamente alta. Esto indica que existe una relación lineal fuerte entre estas variables: generalmente, a medida que una aumenta, la otra también lo hace de manera proporcional y predecible. \n",
    "\n",
    "Este tipo de correlación tan marcada es menos común en muchos fenómenos naturales, pero puede encontrarse en situaciones donde dos variables están intrínsecamente conectadas, como podría ser la relación entre variables genéticas y ciertos rasgos físicos en estudios biomédicos, o entre variables económicas fuertemente interdependientes en estudios de mercado.\n",
    "\n",
    "Es importante recordar que una alta correlación no implica causalidad; es decir, no podemos concluir definitivamente que cambios en una variable causan cambios en la otra sin un análisis más profundo y controlado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los datos\n",
    "plt.scatter(pesos_nuevos, alturas_nuevas)\n",
    "plt.title(\"Diagrama de dispersión de Peso vs. Altura\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Altura (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación de Altura con Correlación Negativa\n",
    "\n",
    "Ahora vamos a explorar el otro extremo del espectro de correlación, generando datos de altura que tengan una correlación de -1 con el peso. Una correlación de -1 indica una relación lineal perfectamente negativa, es decir, a medida que una variable aumenta, la otra disminuye proporcionalmente.\n",
    "\n",
    "## Aplicación\n",
    "\n",
    "Este tipo de correlación es útil para estudiar fenómenos donde se espera que el aumento en una variable esté asociado con una disminución equivalente en otra. En la práctica, este escenario es menos común pero crucial en ciertos contextos científicos y económicos, como podría ser la relación entre la oferta y el precio en mercados altamente competitivos.\n",
    "\n",
    "Vamos a simular estos datos ajustando nuestra serie de pesos de manera que, a medida que el peso aumenta, la altura disminuye. Esto nos permitirá visualizar y comprender mejor cómo se representa gráficamente una correlación perfectamente negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir ruido aleatorio con una desviación estándar de 5 para simular variaciones reales\n",
    "ruido = np.random.normal(loc=0, scale=5, size=500)\n",
    "\n",
    "# Generar alturas con una correlación negativa fuerte respecto a los pesos\n",
    "# Invertimos la relación de pesos a alturas y añadimos ruido\n",
    "alturas_negativas = np.round(250 - 0.9 * pesos_nuevos + ruido, 1)\n",
    "\n",
    "# Calcular la correlación para confirmar que es negativa y fuerte\n",
    "correlacion_negativa = np.corrcoef(pesos_nuevos, alturas_negativas)[0, 1]\n",
    "\n",
    "print(\"Alturas generadas con correlación negativa y ruido:\", alturas_negativas[:10])\n",
    "print(\"Correlación confirmada:\", correlacion_negativa)\n",
    "\n",
    "# Graficar los datos\n",
    "plt.scatter(pesos_nuevos, alturas_negativas)\n",
    "plt.title(\"Diagrama de dispersión de Peso vs. Altura con Correlación Negativa\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Altura (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación de Altura Sin Correlación con el Peso\n",
    "\n",
    "Finalmente, exploraremos un escenario donde las alturas generadas no tienen correlación con los pesos. Una correlación cercana a cero indica que no existe una relación lineal apreciable entre las variables, es decir, los cambios en una variable no predicen de manera confiable los cambios en la otra.\n",
    "\n",
    "## Contexto\n",
    "\n",
    "En el análisis de datos, a menudo encontramos variables que parecen estar relacionadas, pero un análisis estadístico puede revelar que no hay correlación significativa entre ellas. Este es un concepto importante para entender, ya que ayuda a identificar relaciones verdaderamente significativas y evitar conclusiones erróneas sobre la causalidad.\n",
    "\n",
    "## Implementación\n",
    "\n",
    "Para simular datos que reflejen una correlación nula entre peso y altura, generaremos alturas desde una distribución normal que sea independiente de la distribución de pesos. Esto nos permitirá visualizar cómo se distribuyen los puntos en un diagrama de dispersión cuando no hay una relación lineal entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar alturas desde una distribución normal independiente\n",
    "alturas_independientes = np.round(np.random.normal(loc=170, scale=10, size=500), 1)\n",
    "\n",
    "# Calcular la correlación para confirmar que es cercana a cero\n",
    "correlacion_independiente = np.corrcoef(pesos_nuevos, alturas_independientes)[0, 1]\n",
    "\n",
    "print(\"Alturas generadas independientemente:\", alturas_independientes[:10])\n",
    "print(\"Correlación confirmada:\", correlacion_independiente)\n",
    "\n",
    "# Graficar los datos\n",
    "plt.scatter(pesos_nuevos, alturas_independientes)\n",
    "plt.title(\"Diagrama de dispersión de Peso vs. Altura Sin Correlación\")\n",
    "plt.xlabel(\"Peso (kg)\")\n",
    "plt.ylabel(\"Altura (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causalidad vs. Correlación\n",
    "\n",
    "Uno de los conceptos más importantes en estadística y ciencia de datos es la distinción entre causalidad y correlación. Comprender esta diferencia es crucial para evitar interpretaciones erróneas de los datos analizados.\n",
    "\n",
    "## ¿Qué es Correlación?\n",
    "\n",
    "La correlación entre dos variables indica que existe una relación o asociación entre ellas. Por ejemplo, podemos encontrar que hay una correlación positiva entre el consumo de helado y el número de ahogamientos. Esto significa que cuando aumenta el consumo de helado, también aumentan los ahogamientos.\n",
    "\n",
    "## ¿Qué es Causalidad?\n",
    "\n",
    "La causalidad va un paso más allá, sugiriendo que un cambio en una variable causa un cambio en otra. Siguiendo el ejemplo anterior, decir que el consumo de helado causa ahogamientos sería incorrecto y probablemente falso.\n",
    "\n",
    "## Correlación no Implica Causalidad\n",
    "\n",
    "El hecho de que dos variables estén correlacionadas no significa que una cause la otra. En nuestro ejemplo, tanto el consumo de helado como los ahogamientos pueden aumentar durante el verano debido al calor, lo que significa que un tercer factor (el clima más cálido) influye en ambos.\n",
    "\n",
    "Este es un ejemplo clásico de una **variable confusa**, que puede llevar a conclusiones erróneas si no se identifica y controla adecuadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: Cálculo y Visualización de una Matriz de Correlaciones\n",
    "\n",
    "## Objetivo\n",
    "Aplicar los conocimientos adquiridos sobre correlación para calcular todas las combinaciones posibles de correlaciones entre un conjunto de variables y organizarlas en una matriz de correlaciones, representando posteriormente esta matriz de manera visual.\n",
    "\n",
    "## ¿Qué es una Matriz de Correlaciones?\n",
    "Una matriz de correlaciones es una tabla que muestra los coeficientes de correlación entre diferentes variables. Cada celda en la matriz representa la correlación entre dos variables específicas. Por ejemplo:\n",
    "- La celda en la posición `(1, 2)` muestra la correlación entre la Variable 1 y la Variable 2.\n",
    "- La diagonal principal siempre tiene valores de 1, ya que una variable está perfectamente correlacionada consigo misma.\n",
    "\n",
    "### Ejemplo de una matriz de correlaciones\n",
    "Supongamos que tenemos 3 variables: A, B y C. La matriz de correlaciones podría lucir así:\n",
    "\n",
    "|       | A    | B    | C    |\n",
    "|-------|------|------|------|\n",
    "| **A** | 1.00 | 0.85 | 0.12 |\n",
    "| **B** | 0.85 | 1.00 | -0.20 |\n",
    "| **C** | 0.12 | -0.20| 1.00 |\n",
    "\n",
    "- **Valores cercanos a +1**: Relación lineal positiva fuerte.\n",
    "- **Valores cercanos a -1**: Relación lineal negativa fuerte.\n",
    "- **Valores cercanos a 0**: Poca o ninguna relación lineal.\n",
    "\n",
    "## Descripción de la Tarea\n",
    "1. **Cálculo de Correlaciones**:\n",
    "   - Usando la fórmula del coeficiente de correlación de Pearson vista en clase, calcula la correlación entre cada par de variables.\n",
    "   - Repite este cálculo para todas las combinaciones de variables.\n",
    "\n",
    "2. **Organización de Resultados**:\n",
    "   - Una vez calculadas todas las correlaciones, organiza los resultados en forma de matriz.\n",
    "   - Asegúrate de que la diagonal principal contenga valores de 1 (una variable siempre está perfectamente correlacionada consigo misma).\n",
    "\n",
    "3. **Visualización**:\n",
    "   - Usa una librería de gráficos como `matplotlib` para representar la matriz como un mapa de colores (heatmap).\n",
    "\n",
    "## Datos Proporcionados\n",
    "Se te proporcionan 10 series de datos simuladas, representando diferentes variables. Estos datos son los siguientes:\n",
    "\n",
    "| Variable | Valores (Ejemplo)                                                                 |\n",
    "|----------|-----------------------------------------------------------------------------------|\n",
    "| Var1     | 65, 70, 68, 72, 74, 67, 69, 71, 73, 66, 75, 77, 79, 78, 76, 68, 69, 70, 73, 74    |\n",
    "| Var2     | 50, 48, 49, 52, 54, 51, 47, 53, 55, 50, 49, 48, 52, 50, 53, 51, 55, 54, 53, 49    |\n",
    "| Var3     | 40, 42, 41, 39, 38, 43, 37, 44, 36, 45, 42, 40, 41, 43, 44, 38, 36, 37, 45, 39    |\n",
    "| Var4     | 80, 75, 78, 82, 85, 79, 77, 76, 84, 83, 81, 78, 76, 82, 79, 85, 84, 77, 75, 80    |\n",
    "| Var5     | 90, 88, 87, 91, 92, 86, 85, 93, 89, 94, 87, 88, 90, 91, 93, 92, 85, 89, 86, 94    |\n",
    "| Var6     | 30, 28, 27, 32, 31, 29, 26, 33, 25, 34, 32, 30, 29, 28, 33, 27, 31, 34, 26, 25    |\n",
    "| Var7     | 70, 68, 69, 72, 74, 66, 65, 73, 67, 75, 68, 72, 70, 74, 73, 66, 67, 65, 75, 69    |\n",
    "| Var8     | 60, 58, 59, 61, 62, 57, 56, 63, 55, 64, 58, 60, 62, 59, 63, 56, 64, 55, 57, 61    |\n",
    "| Var9     | 20, 18, 19, 21, 22, 17, 16, 23, 15, 24, 18, 21, 20, 19, 22, 16, 15, 23, 17, 24    |\n",
    "| Var10    | 10, 12, 11, 13, 14, 9, 8, 15, 7, 16, 12, 14, 10, 11, 13, 8, 7, 15, 9, 16          |\n",
    "\n",
    "## Actividades\n",
    "1. **Cargar los datos**:\n",
    "   - Define las variables como listas en Python.\n",
    "\n",
    "2. **Cálculo manual de las correlaciones**:\n",
    "   - Implementa la fórmula de correlación de Pearson para calcular la correlación entre cada par de variables. Usa herramientas básicas de Python o NumPy.\n",
    "\n",
    "3. **Construcción de la matriz de correlaciones**:\n",
    "   - Organiza los resultados en una matriz 10x10.\n",
    "\n",
    "4. **Visualización**:\n",
    "   - Representa gráficamente la matriz de correlaciones usando un mapa de colores (heatmap). Asegúrate de incluir etiquetas para cada variable en los ejes del gráfico.\n",
    "\n",
    "## Resultados Esperados\n",
    "1. Código funcional en Python que realice todos los pasos indicados.\n",
    "2. Una imagen o captura de la matriz de correlaciones visualizada como un mapa de colores.\n",
    "3. Un breve análisis de los resultados:\n",
    "   - Identifica pares de variables con alta correlación (positiva o negativa).\n",
    "   - Señala variables que no tienen una relación lineal significativa con otras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
