{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79546cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T19:07:15.140799Z",
     "start_time": "2023-03-31T19:07:15.128768Z"
    }
   },
   "source": [
    "# Reducción de Dimensiones\n",
    "\n",
    "La reducción de dimensiones es una técnica utilizada en el análisis de datos y aprendizaje automático para simplificar conjuntos de datos de alta dimensionalidad. La idea principal es transformar los datos de alta dimensionalidad en una representación de menor dimensionalidad, manteniendo la mayor cantidad de información posible.\n",
    "\n",
    "## ¿Por qué es importante la reducción de dimensiones?\n",
    "\n",
    "1. **Visualización**: Facilita la visualización de datos en 2D o 3D, lo que permite una mejor comprensión e interpretación de los datos.\n",
    "\n",
    "2. **Eficiencia computacional**: Al reducir la dimensionalidad, se disminuye la cantidad de datos a procesar, lo que puede resultar en una mejora en la eficiencia computacional de los algoritmos de aprendizaje automático.\n",
    "\n",
    "3. **Eliminar ruido**: La reducción de dimensiones puede ayudar a eliminar el ruido presente en los datos, ya que las dimensiones irrelevantes o redundantes pueden ser eliminadas en el proceso de transformación.\n",
    "\n",
    "4. **Combatir la maldición de la dimensionalidad**: Con conjuntos de datos de alta dimensionalidad, la cantidad de datos necesarios para entrenar un modelo aumenta exponencialmente. Reducir la dimensionalidad puede ayudar a combatir este problema, haciendo que los modelos sean más eficientes y precisos.\n",
    "\n",
    "## Técnicas de reducción de dimensiones\n",
    "\n",
    "Hay varias técnicas de reducción de dimensiones, y cada una tiene sus propias ventajas y desventajas. Algunas de las técnicas más comunes son:\n",
    "\n",
    "1. **Análisis de Componentes Principales (PCA)**: PCA es una técnica lineal que busca encontrar las direcciones en las que los datos tienen la mayor varianza y proyecta los datos en un espacio de menor dimensionalidad formado por estas direcciones.\n",
    "\n",
    "2. **Análisis Discriminante Lineal (LDA)**: LDA es una técnica supervisada que busca encontrar un espacio de menor dimensionalidad en el que se maximice la separación entre diferentes clases.\n",
    "\n",
    "3. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: t-SNE es una técnica no lineal que busca encontrar una representación de menor dimensionalidad de los datos de tal manera que se conserven las relaciones de proximidad entre puntos en el espacio original.\n",
    "\n",
    "4. **Autoencoders**: Los autoencoders son redes neuronales artificiales que aprenden a comprimir y descomprimir datos en un espacio de menor dimensionalidad.\n",
    "\n",
    "En la siguiente sección, exploraremos el Análisis de Componentes Principales (PCA) en detalle y veremos cómo implementarlo en Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dc1fd",
   "metadata": {},
   "source": [
    "# Análisis de Componentes Principales (PCA)\n",
    "\n",
    "El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica de reducción de dimensiones que busca transformar un conjunto de datos de alta dimensionalidad en uno de menor dimensionalidad, manteniendo la mayor cantidad de información posible. PCA es una técnica lineal y no supervisada.\n",
    "\n",
    "## ¿Cómo funciona PCA?\n",
    "\n",
    "PCA sigue los siguientes pasos para transformar los datos:\n",
    "\n",
    "1. **Estandarización**: Los datos se estandarizan para que cada característica tenga media 0 y desviación estándar 1. Esto es necesario porque PCA es sensible a las escalas de las variables.\n",
    "\n",
    "2. **Cálculo de la matriz de covarianza**: Se calcula la matriz de covarianza de los datos estandarizados. La matriz de covarianza contiene información sobre las correlaciones entre las diferentes características.\n",
    "\n",
    "3. **Cálculo de los eigenvectores y eigenvalores**: Se calculan los eigenvectores y eigenvalores de la matriz de covarianza. Los eigenvectores representan las direcciones en las que los datos tienen la mayor varianza, mientras que los eigenvalores representan la magnitud de la varianza en esas direcciones.\n",
    "\n",
    "4. **Selección de componentes principales**: Se seleccionan los k eigenvectores con los k eigenvalores más grandes para formar una matriz de proyección de k dimensiones.\n",
    "\n",
    "5. **Proyección de los datos**: Se proyectan los datos estandarizados en el espacio de menor dimensionalidad formado por los k componentes principales.\n",
    "\n",
    "## Interpretación de los resultados\n",
    "\n",
    "Los componentes principales resultantes son combinaciones lineales de las características originales y son ortogonales entre sí. Estos componentes principales capturan la mayor cantidad de varianza de los datos en el espacio de menor dimensionalidad.\n",
    "\n",
    "El primer componente principal es la dirección que captura la mayor cantidad de varianza en los datos, el segundo componente principal es la dirección ortogonal al primer componente principal que captura la segunda mayor cantidad de varianza, y así sucesivamente.\n",
    "\n",
    "La cantidad de varianza explicada por cada componente principal se puede calcular como la proporción del eigenvalor correspondiente al total de los eigenvalores. Se puede utilizar la suma acumulada de las proporciones de varianza explicada para determinar cuántos componentes principales se deben conservar en la transformación.\n",
    "\n",
    "En la siguiente sección, veremos cómo implementar PCA en Python utilizando la biblioteca de scikit-learn y cómo interpretar los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd53e6",
   "metadata": {},
   "source": [
    "# Ejemplo: PCA con el conjunto de datos Breast Cancer Wisconsin\n",
    "\n",
    "En este ejemplo, aplicaremos PCA al conjunto de datos Breast Cancer Wisconsin para reducir su dimensionalidad y visualizar la separación entre clases en un espacio de menor dimensionalidad. Este conjunto de datos contiene 569 instancias y 30 características numéricas calculadas a partir de imágenes digitalizadas de aspiración con aguja fina de masas mamarias. El objetivo es clasificar si los tumores son benignos o malignos.\n",
    "\n",
    "## Cargando y explorando el conjunto de datos\n",
    "\n",
    "Comencemos cargando el conjunto de datos y realizando una exploración básica de sus características y etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa9eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:07:15.859153Z",
     "start_time": "2023-03-31T22:07:14.708521Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Ver las primeras filas del conjunto de datos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da213ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec348957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:19:39.857049Z",
     "start_time": "2023-03-31T22:19:39.840062Z"
    }
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff3fc8",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "Antes de aplicar PCA, es necesario estandarizar los datos para que cada característica tenga media 0 y desviación estándar 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59125c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:12:35.997728Z",
     "start_time": "2023-03-31T22:12:35.988309Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c339e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:15:18.252561Z",
     "start_time": "2023-03-31T22:15:18.235077Z"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69517c18",
   "metadata": {},
   "source": [
    "## Aplicando PCA\n",
    "Ahora, vamos a aplicar PCA utilizando scikit-learn y proyectar los datos en 2 dimensiones para facilitar la visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55132a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:18:05.906673Z",
     "start_time": "2023-03-31T22:18:05.881670Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Aplicar PCA y reducir la dimensionalidad a 2\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Crear un DataFrame con los datos transformados y las etiquetas\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['target'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c769b0b",
   "metadata": {},
   "source": [
    "# Cómo decidir cuáles componentes principales escoger\n",
    "\n",
    "Al aplicar PCA, es importante determinar cuántas componentes principales seleccionar para lograr un equilibrio entre la reducción de dimensionalidad y la conservación de la información relevante de los datos originales. Una forma común de decidir esto es utilizando la proporción de varianza explicada por cada componente principal.\n",
    "\n",
    "## Proporción de varianza explicada\n",
    "\n",
    "La proporción de varianza explicada nos indica cuánta varianza de los datos originales es capturada por cada componente principal. Podemos calcularla utilizando la propiedad `explained_variance_ratio_` del objeto PCA de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f35cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:24:45.881091Z",
     "start_time": "2023-03-31T22:24:45.845620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar PCA sin reducir el número de componentes\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Calcular la proporción de varianza explicada por cada componente principal\n",
    "explained_variance_ratio_full = pca_full.explained_variance_ratio_\n",
    "print(\"Proporción de varianza explicada por cada componente principal:\\n\", explained_variance_ratio_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecb2f1",
   "metadata": {},
   "source": [
    "## Gráfica de la proporción acumulada de varianza explicada\n",
    "Una forma visual de decidir cuántas componentes principales seleccionar es trazar la proporción acumulada de varianza explicada en función del número de componentes principales. Esta gráfica también se conoce como \"gráfica de codo\" o \"gráfica de Scree\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe4232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:29:24.104174Z",
     "start_time": "2023-03-31T22:29:22.597133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la proporción acumulada de varianza explicada\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio_full)\n",
    "\n",
    "# Crear un gráfico de la proporción acumulada de varianza explicada\n",
    "fig = px.line(x=range(1, len(cumulative_explained_variance_ratio) + 1), y=cumulative_explained_variance_ratio, markers=True)\n",
    "fig.update_xaxes(title_text='Número de componentes principales')\n",
    "fig.update_yaxes(title_text='Proporción acumulada de varianza explicada')\n",
    "fig.update_layout(title='Gráfica de la proporción acumulada de varianza explicada')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba9076",
   "metadata": {},
   "source": [
    "Al observar la gráfica, podemos buscar un \"codo\" o un punto de inflexión donde la proporción acumulada de varianza explicada deja de aumentar significativamente con la adición de más componentes principales. Este punto puede ser un buen indicador de cuántas componentes principales seleccionar.\n",
    "\n",
    "Sin embargo, es importante tener en cuenta que no siempre hay un \"codo\" claramente definido y que la elección del número de componentes principales puede depender del contexto y de los objetivos específicos del análisis. En algunos casos, también puede ser útil utilizar técnicas de validación cruzada o pruebas de rendimiento en algoritmos de aprendizaje automático para determinar el número óptimo de componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e24ffb",
   "metadata": {},
   "source": [
    "## Visualización de los resultados\n",
    "Utilizaremos plotly para visualizar la proyección de los datos en las dos primeras componentes principales y observar la separación entre clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c98a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:32:38.985330Z",
     "start_time": "2023-03-31T22:32:38.869255Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_pca, x='PC1', y='PC2', color=df_pca['target'].astype(str),\n",
    "                 labels={'color': 'Target'}, width=800, height=600)  # Ajusta el tamaño del gráfico\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Proyección de los datos en las dos primeras componentes principales',\n",
    "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),  # Esto asegura que los ejes x e y tengan la misma escala\n",
    "    xaxis_title=\"PC1\",  # Etiqueta del eje X\n",
    "    yaxis_title=\"PC2\",  # Etiqueta del eje Y\n",
    "    legend_title=\"Target\"  # Título de la leyenda\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666b24a",
   "metadata": {},
   "source": [
    "## Interpretación de los resultados\n",
    "La gráfica muestra la proyección de los datos en las dos primeras componentes principales. Podemos observar cierta separación entre las clases (tumores benignos y malignos), lo que sugiere que PCA ha logrado reducir la dimensionalidad mientras mantiene información útil para la clasificación.\n",
    "\n",
    "Para entender cuánta varianza de los datos ha sido capturada por las componentes principales, podemos calcular la proporción de varianza explicada por cada componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3c726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:34:49.725969Z",
     "start_time": "2023-03-31T22:34:49.705940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la proporción de varianza explicada por cada componente principal\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Proporción de varianza explicada por las dos primeras componentes principales:\", explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a88d0",
   "metadata": {},
   "source": [
    "Esto nos dará una idea de qué tan representativas son las dos primeras componentes principales en términos de la varianza total de los datos originales. Si deseamos aumentar la cantidad de varianza explicada, podríamos considerar aumentar el número de componentes principales en nuestra proyección.\n",
    "\n",
    "## Visualización en 3D\n",
    "Si deseamos visualizar la proyección de los datos en tres dimensiones en lugar de dos, podemos aplicar PCA con n_components=3 y crear un gráfico de dispersión 3D utilizando plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e4e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:38:07.012810Z",
     "start_time": "2023-03-31T22:38:06.847819Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Crear un DataFrame con los datos transformados y las etiquetas\n",
    "df_pca_3d = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_3d['target'] = data.target  # Asegúrate de que 'data.target' sea el vector correcto de etiquetas\n",
    "\n",
    "# Crear un gráfico de dispersión 3D de las componentes principales\n",
    "fig = px.scatter_3d(df_pca_3d, x='PC1', y='PC2', z='PC3', color=df_pca_3d['target'].astype(str),\n",
    "                    labels={'color': 'Target'}, width=800, height=600)  # Ajusta el tamaño del gráfico\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Proyección de los datos en las tres primeras componentes principales',\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    legend_title=\"Target\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf572",
   "metadata": {},
   "source": [
    "Esta visualización en 3D puede proporcionar una perspectiva adicional sobre la estructura de los datos y la separación entre clases. De manera similar a la visualización en 2D, también podemos calcular la proporción de varianza explicada por las tres primeras componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bce55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:39:31.175967Z",
     "start_time": "2023-03-31T22:39:31.168973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la proporción de varianza explicada por las tres primeras componentes principales\n",
    "explained_variance_ratio_3d = pca_3d.explained_variance_ratio_\n",
    "print(\"Proporción de varianza explicada por las tres primeras componentes principales:\", explained_variance_ratio_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712df40",
   "metadata": {},
   "source": [
    "Hasta este punto hemos aplicado PCA al conjunto de datos Breast Cancer Wisconsin, reduciendo su dimensionalidad y visualizando la proyección de los datos en dos y tres dimensiones. A través de la visualización, hemos observado cierta separación entre clases, lo que sugiere que PCA puede ser útil para simplificar la información contenida en este conjunto de datos antes de aplicar algoritmos de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518a6db",
   "metadata": {},
   "source": [
    "# Aplicando K-means al conjunto de datos Breast Cancer Wisconsin\n",
    "Después de reducir la dimensionalidad del conjunto de datos utilizando PCA, podemos aplicar K-means para realizar un análisis de clustering. K-means es un algoritmo de clustering que intenta dividir el conjunto de datos en K grupos (clusters) disjuntos, minimizando la suma de las distancias al cuadrado entre los puntos y el centroide de su cluster correspondiente. \n",
    "\n",
    "En este caso, utilizaremos el objeto `KMeans` de scikit-learn para aplicar el algoritmo K-means al conjunto de datos Breast Cancer Wisconsin reducido a tres dimensiones con PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c18798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:45:44.858099Z",
     "start_time": "2023-03-31T22:45:44.495666Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Aplicar K-means con 2 clusters al conjunto de datos reducido a 3 dimensiones\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(X_pca_3d)\n",
    "\n",
    "# Crear un DataFrame con los datos reducidos y las etiquetas de clustering\n",
    "dataset_pca_3d = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "dataset_pca_3d['Cluster'] = kmeans_labels.astype(str)  # Convertir a string para mejorar la visualización de colores categóricos\n",
    "\n",
    "# Crear una visualización 3D de los datos y los clusters\n",
    "fig = px.scatter_3d(dataset_pca_3d, x='PC1', y='PC2', z='PC3', color='Cluster',\n",
    "                    title='Clustering de K-means aplicado al conjunto de datos Breast Cancer Wisconsin en R3',\n",
    "                    labels={'Cluster': 'Cluster ID'}, width=800, height=600)\n",
    "\n",
    "# Ajustar la transparencia y el tamaño del marker para los nodos\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.5))  # Reducir el tamaño y añadir transparencia\n",
    "\n",
    "# Añadir los centroides de los clusters al gráfico\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['PC1', 'PC2', 'PC3'])\n",
    "fig.add_scatter3d(x=centroids['PC1'], y=centroids['PC2'], z=centroids['PC3'], mode='markers',\n",
    "                  marker=dict(size=5, color='black', symbol='x'), name='Centroides')  # Tamaño reducido para los centroides\n",
    "\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a43e6c",
   "metadata": {},
   "source": [
    "## Comparación de los resultados del clustering con las etiquetas reales del dataset\n",
    "Para evaluar la efectividad del clustering de K-means, podemos comparar los resultados del clustering con las etiquetas reales del conjunto de datos Breast Cancer Wisconsin. Dado que las etiquetas de clustering no tienen un orden específico, es posible que necesitemos ajustarlas antes de compararlas con las etiquetas reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166b3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:47:10.582136Z",
     "start_time": "2023-03-31T22:47:10.573101Z"
    }
   },
   "outputs": [],
   "source": [
    "real_labels = df_pca_3d['target'].values\n",
    "real_labels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109d108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:47:29.730804Z",
     "start_time": "2023-03-31T22:47:29.709812Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_labels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb41430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:48:58.774831Z",
     "start_time": "2023-03-31T22:48:58.757853Z"
    }
   },
   "outputs": [],
   "source": [
    "def reetiquetar_clusters(etiquetas, mapeo):\n",
    "    etiquetas_reetiquetadas = np.array([mapeo[etiqueta] for etiqueta in etiquetas])\n",
    "    return etiquetas_reetiquetadas\n",
    "\n",
    "# Ajustar las etiquetas de clustering para que coincidan con las etiquetas reales\n",
    "# Define el diccionario de mapeo para reetiquetar los valores de los clusters\n",
    "mapeo = {0: 1, 1: 0}\n",
    "\n",
    "# Reetiqueta las etiquetas de los clusters usando la función reetiquetar_clusters\n",
    "adjusted_kmeans_labels = reetiquetar_clusters(kmeans_labels, mapeo)\n",
    "adjusted_kmeans_labels[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23670340",
   "metadata": {},
   "source": [
    "## Matriz de confusión\n",
    "La matriz de confusión es una tabla que se utiliza para describir el rendimiento de un algoritmo de clasificación en un conjunto de datos para el cual se conocen las etiquetas verdaderas. La matriz de confusión compara las etiquetas predichas por el algoritmo con las etiquetas reales. En el caso de un problema de clasificación binaria, la matriz de confusión es una matriz 2x2 que consta de los siguientes elementos:\n",
    "\n",
    "* Verdaderos positivos (TP): Número de casos en los que el algoritmo predijo correctamente la clase positiva.\n",
    "* Verdaderos negativos (TN): Número de casos en los que el algoritmo predijo correctamente la clase negativa.\n",
    "* Falsos positivos (FP): Número de casos en los que el algoritmo predijo incorrectamente la clase positiva (error de tipo I).\n",
    "* Falsos negativos (FN): Número de casos en los que el algoritmo predijo incorrectamente la clase negativa (error de tipo II).\n",
    "\n",
    "En el caso de un problema de clasificación multiclase, la matriz de confusión es una matriz cuadrada NxN, donde N es el número de clases. Cada fila de la matriz representa las instancias de una clase real, mientras que cada columna representa las instancias de una clase predicha. Los elementos de la diagonal principal de la matriz indican las clasificaciones correctas, mientras que los elementos fuera de la diagonal indican las clasificaciones incorrectas.\n",
    "\n",
    "## Precisión (accuracy)\n",
    "La precisión (accuracy) es una métrica de evaluación del rendimiento que se utiliza para medir qué tan bien un algoritmo de clasificación ha predicho las etiquetas correctas en un conjunto de datos. La precisión se calcula dividiendo el número total de predicciones correctas (tanto verdaderos positivos como verdaderos negativos) por el número total de casos en el conjunto de datos.\n",
    "\n",
    "En términos de la matriz de confusión, la precisión se calcula como:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a017f398",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e25768",
   "metadata": {},
   "source": [
    "La precisión es una métrica útil cuando las clases en el conjunto de datos están equilibradas y se quiere evaluar el rendimiento general del algoritmo. Sin embargo, la precisión puede ser engañosa si las clases están desequilibradas, ya que un algoritmo que siempre predice la clase mayoritaria puede obtener una alta precisión sin realmente clasificar bien los casos de la clase minoritaria. En tales casos, otras métricas como la precisión, el recall y la puntuación F1 pueden proporcionar una evaluación más completa del rendimiento del algoritmo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf38f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T22:54:26.745652Z",
     "start_time": "2023-03-31T22:54:26.715808Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calcular la matriz de confusión y la precisión del clustering\n",
    "conf_matrix = confusion_matrix(real_labels, adjusted_kmeans_labels)\n",
    "accuracy = accuracy_score(real_labels, adjusted_kmeans_labels)\n",
    "\n",
    "print(\"Matriz de confusión:\\n\", conf_matrix)\n",
    "print(\"Precisión del clustering:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ebd5c",
   "metadata": {},
   "source": [
    "## Ejercicio: Análisis de componentes principales y clustering K-means en datos de leucemia\n",
    "\n",
    "Se les proporciona un conjunto de datos que contiene información sobre la expresión de 50 genes relacionados con la leucemia en 38 pacientes. En teoría, hay dos tipos de leucemia presentes en el conjunto de datos. Se les pide que realicen las siguientes tareas:\n",
    "\n",
    "1. Cargue el conjunto de datos \"Leucemia\" y explore su contenido.\n",
    "2. Realice un análisis de componentes principales (PCA) en los datos para reducir la dimensionalidad. Decida cuántos componentes principales retener basándose en el criterio de varianza explicada acumulada.\n",
    "3. Visualice los datos reducidos en un gráfico de dispersión, utilizando los componentes principales seleccionados. ¿Puede identificar visualmente dos grupos diferentes en los datos?\n",
    "4. Aplique el algoritmo de clustering K-means en los datos reducidos, utilizando el número adecuado de clusters (en este caso, 2 clusters).\n",
    "5. Visualice los clusters resultantes en un gráfico de dispersión, utilizando los componentes principales seleccionados. Coloree los puntos según la etiqueta del cluster asignada por K-means.\n",
    "6. Analice los resultados y discuta si los clusters encontrados parecen corresponder a los dos tipos de leucemia presentes en el conjunto de datos.\n",
    "\n",
    "Para este ejercicio, pueden utilizar las bibliotecas `numpy`, `pandas`, `plotly`, `scikit-learn` u otras bibliotecas relevantes en Python. Recuerden seguir las buenas prácticas en la organización del código y la documentación de los pasos realizados. Buena suerte y diviértanse explorando los datos y aplicando las técnicas de reducción de dimensiones y clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f062cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T23:02:29.875930Z",
     "start_time": "2023-03-31T23:02:29.843540Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_leucemia=pd.read_csv(\"leucemia_dataset.csv\",header=None)\n",
    "x=df_leucemia.values.T\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a5230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T23:02:48.347315Z",
     "start_time": "2023-03-31T23:02:48.240316Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(x)\n",
    "\n",
    "# Configurar el layout para mantener la misma escala en ambos ejes\n",
    "fig.update_layout(\n",
    "    title=\"Visualización de Matriz\",\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),  \n",
    "    yaxis=dict(constrain='domain')  # Establece la misma longitud de dominio para el eje Y\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(x_scaled)\n",
    "\n",
    "# Configurar el layout para mantener la misma escala en ambos ejes\n",
    "fig.update_layout(\n",
    "    title=\"Visualización de Matriz\",\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),  \n",
    "    yaxis=dict(constrain='domain')  # Establece la misma longitud de dominio para el eje Y\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(x_scaled)\n",
    "\n",
    "# Aplicar K-means con 2 clusters al conjunto de datos reducido a 3 dimensiones\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(X_pca_3d)\n",
    "\n",
    "# Crear un DataFrame con los datos reducidos y las etiquetas de clustering\n",
    "dataset_pca_3d = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "dataset_pca_3d['Cluster'] = kmeans_labels.astype(str)  # Convertir a string para mejorar la visualización de colores categóricos\n",
    "\n",
    "# Crear una visualización 3D de los datos y los clusters\n",
    "fig = px.scatter_3d(dataset_pca_3d, x='PC1', y='PC2', z='PC3', color='Cluster',\n",
    "                    title='Clustering de K-means aplicado al conjunto de datos Leucemia en R3',\n",
    "                    labels={'Cluster': 'Cluster ID'}, width=800, height=600)\n",
    "\n",
    "# Añadir los centroides de los clusters al gráfico\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['PC1', 'PC2', 'PC3'])\n",
    "fig.add_scatter3d(x=centroids['PC1'], y=centroids['PC2'], z=centroids['PC3'], mode='markers',\n",
    "                  marker=dict(size=5, color='black', symbol='x'), name='Centroides')\n",
    "\n",
    "fig.update_layout(legend_title_text='Cluster ID')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
