{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Random Forest\n",
    "\n",
    "El algoritmo de Random Forest es un método de aprendizaje conjunto para clasificación, regresión y otras tareas que opera construyendo una multitud de árboles de decisión en el momento del entrenamiento y generando la clase que es la moda de las clasificaciones (clasificación) o predicción media (regresión) de los árboles individuales.\n",
    "\n",
    "## ¿Cómo funciona?\n",
    "\n",
    "Random Forest mejora el modelo de un solo árbol de decisión al agregar un elemento de aleatoriedad durante la construcción del modelo. El algoritmo sigue estos pasos:\n",
    "\n",
    "1. **Bootstrap sampling**: Selecciona aleatoriamente muestras del conjunto de datos con reemplazo para construir múltiples conjuntos de entrenamiento. Esto se conoce como 'bagging' o 'Bootstrap Aggregating'.\n",
    "\n",
    "2. **Construcción de árboles**: Para cada muestra de entrenamiento generada, se construye un árbol de decisión. Para aumentar la diversidad, cada árbol se construye de la siguiente manera:\n",
    "   - En cada división, se selecciona un subconjunto aleatorio de las características (variables).\n",
    "   - Se elige la mejor división de este subconjunto (no de todas las características) para dividir el nodo.\n",
    "   - Cada árbol se desarrolla hasta el máximo, sin podar.\n",
    "\n",
    "3. **Predicción**:\n",
    "   - En el caso de la **clasificación**, para una nueva muestra, cada árbol en el bosque predice una clase y la clase con la mayoría de votos se convierte en la predicción del modelo.\n",
    "   - En el caso de la **regresión**, se promedian las predicciones de todos los árboles.\n",
    "\n",
    "## Ventajas de Random Forest\n",
    "\n",
    "- **Robustez**: Al promediar los resultados de varios árboles, reduce la varianza y evita el sobreajuste, lo que lo hace más robusto que un único árbol de decisión.\n",
    "- **Manejo de datos no lineales**: Puede capturar relaciones no lineales entre las características y la variable de salida.\n",
    "- **Importancia de las características**: Puede proporcionar una estimación de qué variables son importantes en la clasificación.\n",
    "- **Manejo de datos faltantes**: Puede manejar datos faltantes al mantener la precisión cuando una parte de los datos está ausente.\n",
    "\n",
    "## Desventajas de Random Forest\n",
    "\n",
    "- **Complejidad**: Es más complejo y computacionalmente más intensivo que un árbol de decisión.\n",
    "- **Interpretabilidad**: Aunque cada árbol individual es interpretable, el conjunto del bosque puede ser difícil de interpretar.\n",
    "- **Tiempo de entrenamiento**: El entrenamiento puede ser lento, especialmente en conjuntos de datos grandes y complejos.\n",
    "\n",
    "Random Forest es una extensión natural de los árboles de decisión y es útil cuando necesitamos un equilibrio entre facilidad de uso y rendimiento predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Diabetes\n",
    "\n",
    "## Problema a Resolver\n",
    "\n",
    "El objetivo de este análisis es utilizar el algoritmo de Random Forest para predecir si un paciente tiene diabetes, basándose en ciertas medidas diagnósticas incluidas en el conjunto de datos. Se trata de un problema de clasificación binaria, donde el resultado a predecir es si el paciente tiene diabetes (1) o no (0).\n",
    "\n",
    "## Descripción de las Variables\n",
    "\n",
    "El conjunto de datos consta de varias variables médicas predictivas y una variable objetivo:\n",
    "\n",
    "- **Embarazos**: Número de veces que la paciente ha estado embarazada.\n",
    "- **Glucosa**: Concentración de glucosa en plasma a las 2 horas en una prueba oral de tolerancia a la glucosa. Un nivel alto de glucosa en plasma tras 2 horas indica resistencia a la insulina y puede ser signo de diabetes.\n",
    "- **PresionArterial**: Presión arterial diastólica (la presión más baja en las arterias durante el ciclo cardíaco). La hipertensión está asociada con mayores riesgos de diabetes.\n",
    "- **GrosorPiel**: Grosor del pliegue cutáneo del tríceps. Un método para estimar la cantidad de grasa corporal.\n",
    "- **Insulina**: Esta variable mide el nivel de insulina en la sangre dos horas después de que una persona ha ingerido glucosa.\n",
    "- **IMC**: Índice de Masa Corporal. La obesidad es un factor de riesgo significativo para la diabetes.\n",
    "- **FuncionPedigriDiabetes**: Una función que representa la predisposición genética a la diabetes, basada en la historia familiar.\n",
    "- **Edad**: Edad en años. El riesgo de diabetes aumenta con la edad.\n",
    "\n",
    "Con estas variables, desarrollaremos un modelo para predecir el **Resultado**, que indica si la persona tiene diabetes (1) o no (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carga el dataset de diabetes\n",
    "file_path = 'diabetes.csv'\n",
    "diabetes_df = pd.read_csv(file_path)\n",
    "\n",
    "# Muestra las primeras filas del dataframe\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "Antes de sumergirnos en la construcción de modelos de aprendizaje automático, es esencial realizar un Análisis Exploratorio de Datos (EDA). El EDA nos ayudará a entender mejor las características del conjunto de datos, detectar posibles anomalías y formular hipótesis sobre los datos.\n",
    "\n",
    "## Estadísticas Descriptivas\n",
    "\n",
    "Comenzaremos el EDA obteniendo estadísticas descriptivas de nuestro conjunto de datos. Esto incluye contar el número de entradas, calcular la media, mediana, desviación estándar, valores mínimos y máximos, y los cuartiles de cada columna. Esta información es fundamental para tener una visión general de las distribuciones y la escala de cada variable.\n",
    "\n",
    "## Datos Faltantes\n",
    "\n",
    "Tras obtener las estadísticas descriptivas, nuestro siguiente paso es identificar si existen datos faltantes en nuestro conjunto de datos. La presencia de datos faltantes puede afectar el rendimiento de nuestro modelo de Random Forest y sesgar nuestras conclusiones. Por lo tanto, es crucial identificar y decidir cómo manejar estos valores faltantes, ya sea mediante imputación o eliminación de los registros afectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas Descriptivas\n",
    "descriptive_stats = diabetes_df.describe()\n",
    "\n",
    "# Detección de Datos Faltantes\n",
    "missing_values = diabetes_df.isnull().sum()\n",
    "\n",
    "# Visualización de las Estadísticas Descriptivas\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Visualización de los Datos Faltantes\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valores de Cero en el Dataset de Diabetes\n",
    "\n",
    "Al revisar el conjunto de datos de diabetes, notamos la presencia de valores de 0 en varias columnas. Sin embargo, en el contexto médico de este dataset, ciertos valores de 0 no son válidos ya que representan la ausencia de la medida médica en lugar de una medida real de cero. A continuación, se explica cómo se recomienda manejar estos valores de 0 para cada variable:\n",
    "\n",
    "- **Glucosa**: Un nivel de glucosa de 0 no es posible ya que la glucosa siempre está presente en la sangre. Por lo tanto, se recomienda imputar estos valores con una medida central como la media o la mediana de la columna.\n",
    "- **PresionArterial**: Una presión arterial diastólica de 0 no es viable clínicamente. Estos valores deben ser imputados o las observaciones eliminadas.\n",
    "- **GrosorPiel**: Un grosor de piel de 0 no es realista ya que todos los humanos tienen algún grosor de piel, incluso si es delgado. Se deberían imputar estos valores.\n",
    "- **Insulina**: Un nivel de insulina de 0 es inusual y probablemente indica una medición no tomada o perdida. Estos valores también deben ser imputados.\n",
    "- **IMC**: Un Índice de Masa Corporal (IMC) de 0 no es posible; estos valores deben ser imputados.\n",
    "\n",
    "Para las variables **'Embarazos'** y **'Resultado'**, un valor de 0 es válido ya que '0' embarazos es posible y '0' en la columna de resultado indica que no hay presencia de diabetes.\n",
    "\n",
    "## Estrategias de Imputación\n",
    "\n",
    "La estrategia de imputación puede variar dependiendo del patrón de los datos faltantes y el mecanismo subyacente que los produce. Algunas estrategias comunes incluyen:\n",
    "\n",
    "- Imputación con la media o la mediana: útil cuando los datos faltantes son aleatorios y no hay una cantidad excesiva de ellos.\n",
    "- Imputación mediante métodos de modelado: como la regresión lineal o algoritmos de machine learning, que pueden ser más precisos pero también más complejos.\n",
    "- Eliminación de registros: si los datos faltantes no son aleatorios o si la cantidad de datos faltantes es pequeña, se pueden eliminar los registros.\n",
    "\n",
    "Es esencial realizar un análisis cuidadoso para determinar la estrategia más adecuada antes de proceder con cualquier método de imputación o eliminación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación de valores de 0 con la mediana para las columnas especificadas\n",
    "columns_to_impute = ['Glucosa', 'PresionArterial', 'GrosorPiel', 'Insulina', 'IMC']\n",
    "\n",
    "for column in columns_to_impute:\n",
    "    # Calculamos la mediana de la columna, excluyendo los ceros\n",
    "    median_value = diabetes_df[diabetes_df[column] != 0][column].median()\n",
    "    # Reemplazamos los ceros con la mediana\n",
    "    diabetes_df[column] = diabetes_df[column].replace(0, median_value)\n",
    "\n",
    "# Verificación de la imputación\n",
    "imputation_check = diabetes_df[columns_to_impute].min()\n",
    "\n",
    "print(imputation_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización del Uso de Memoria del DataFrame\n",
    "\n",
    "Al trabajar con conjuntos de datos en ciencia de datos y aprendizaje automático, es fundamental gestionar eficientemente los recursos, en particular la memoria. Un uso eficiente de la memoria puede traducirse en un procesamiento más rápido y en la posibilidad de manejar conjuntos de datos más grandes de manera efectiva.\n",
    "\n",
    "## ¿Por qué Optimizar la Memoria?\n",
    "\n",
    "Inicialmente, al cargar nuestro `DataFrame` y utilizar el método `.info()`, observamos que ciertas columnas utilizan tipos de datos (`int64`, `float64`) que pueden ser más grandes de lo necesario. Estos tipos de datos de mayor tamaño ocupan más memoria, lo cual puede no ser eficiente, especialmente cuando los valores reales de los datos pueden representarse con tipos de datos más pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, revisamos los tipos de datos y el uso de memoria\n",
    "diabetes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrategia de Optimización\n",
    "\n",
    "La estrategia para optimizar el uso de memoria implica cambiar los tipos de datos de las columnas a formatos que consuman menos memoria, pero que aún puedan representar todos los valores en la columna. Por ejemplo:\n",
    "\n",
    "- Cambiar de `int64` a `int16` o `int8` cuando los valores máximos y mínimos de la columna se encuentran dentro del rango que estos tipos pueden representar.\n",
    "- Cambiar de `float64` a `float32` para reducir a la mitad el uso de memoria por cada valor de punto flotante.\n",
    "\n",
    "Esta optimización es particularmente útil para mejorar la eficiencia general en el manejo de datos y es una buena práctica en la preparación de datos para el modelado con algoritmos de aprendizaje automático como Random Forest.\n",
    "\n",
    "Al final de este proceso, volveremos a utilizar `.info()` para verificar la reducción efectiva en el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df['Embarazos'] = diabetes_df['Embarazos'].astype('int8')\n",
    "diabetes_df['Glucosa'] = diabetes_df['Glucosa'].astype('int16')\n",
    "diabetes_df['PresionArterial'] = diabetes_df['PresionArterial'].astype('int8')\n",
    "diabetes_df['GrosorPiel'] = diabetes_df['GrosorPiel'].astype('int8')\n",
    "diabetes_df['Insulina'] = diabetes_df['Insulina'].astype('int16')\n",
    "diabetes_df['Edad'] = diabetes_df['Edad'].astype('int8')\n",
    "diabetes_df['Resultado'] = diabetes_df['Resultado'].astype('int8')\n",
    "diabetes_df['IMC'] = diabetes_df['IMC'].astype('float32')\n",
    "diabetes_df['FuncionPedigriDiabetes'] = diabetes_df['FuncionPedigriDiabetes'].astype('float32')\n",
    "\n",
    "# Verificar los cambios realizados\n",
    "print(diabetes_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del Algoritmo Random Forest con Scikit-Learn\n",
    "\n",
    "Para implementar el algoritmo Random Forest en Python, utilizaremos la clase `RandomForestClassifier` de la biblioteca scikit-learn. Esta clase es ampliamente utilizada por su facilidad de uso y sus potentes capacidades.\n",
    "\n",
    "## `RandomForestClassifier` y sus Parámetros\n",
    "\n",
    "`RandomForestClassifier` es una clase que implementa un clasificador de Random Forest, que es un meta estimador que ajusta un número de clasificadores de árboles de decisión en varias submuestras del dataset y utiliza el promedio para mejorar la precisión predictiva y controlar el sobreajuste.\n",
    "\n",
    "### Parámetros Clave y sus Valores Predeterminados\n",
    "\n",
    "- `n_estimators`: Número de árboles en el bosque. Por defecto es 100. Aumentar este número mejora la precisión del modelo pero también aumenta el tiempo de entrenamiento.\n",
    "\n",
    "- `criterion`: La función para medir la calidad de una división. Por defecto es 'gini', aunque también se puede usar 'entropy'. 'gini' tiende a ser más rápido de calcular y 'entropy' puede generar árboles más balanceados.\n",
    "\n",
    "- `max_depth`: La profundidad máxima del árbol. Por defecto es `None`, lo que significa que los nodos se expanden hasta que contienen menos que `min_samples_split` muestras. Limitar la profundidad puede ayudar a prevenir el sobreajuste.\n",
    "\n",
    "- `min_samples_split`: El número mínimo de muestras requeridas para dividir un nodo interno. Por defecto es 2. Valores mayores pueden prevenir el sobreajuste, pero también pueden impedir que el modelo aprenda los datos suficientemente.\n",
    "\n",
    "- `min_samples_leaf`: El número mínimo de muestras requeridas para estar en un nodo hoja. Por defecto es 1. Un número mayor suaviza el modelo, especialmente en regresión.\n",
    "\n",
    "- `max_features`: El número de características a considerar al buscar la mejor división. Por defecto es `auto`, que toma todas las características. Reducir este número puede disminuir la varianza del modelo, pero también su precisión.\n",
    "\n",
    "- `bootstrap`: Indica si se usan muestras de bootstrap para construir árboles. Por defecto es `True`, lo que significa que se utilizan muestras de bootstrap.\n",
    "\n",
    "- `random_state`: Controla la aleatoriedad del bootstrapping y la selección de características. Puede ser un número entero para resultados reproducibles.\n",
    "\n",
    "- `n_jobs`: Especifica el número de trabajos que se ejecutan en paralelo. Por defecto es `None`, lo que significa 1. Establecer en -1 utiliza todos los procesadores disponibles, lo que puede acelerar el entrenamiento.\n",
    "\n",
    "- `class_weight`: Ponderaciones asociadas a clases. Por defecto es `None`, lo que significa que todas las clases tienen el peso uno. Esto puede ser útil en casos de clases desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dividir el dataset en características (X) y la variable objetivo (y)\n",
    "X = diabetes_df.drop('Resultado', axis=1)\n",
    "y = diabetes_df['Resultado']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión y la exactitud\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "\n",
    "# Calcular y visualizar la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Imprimir las métricas de evaluación\n",
    "print(f'Precisión: {precision}')\n",
    "print(f'Exactitud: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de la Matriz de Confusión\n",
    "\n",
    "Recuerda que la matriz de confusión es una herramienta poderosa para evaluar el rendimiento de un modelo de clasificación. Proporciona una visión detallada de cómo el modelo está clasificando las observaciones entre las diferentes categorías.\n",
    "\n",
    "## ¿Qué es la Matriz de Confusión?\n",
    "\n",
    "La matriz de confusión es una tabla que se utiliza para describir el rendimiento de un modelo de clasificación en un conjunto de datos cuyos valores verdaderos son conocidos. En nuestro caso, compara los valores reales de diabetes (positivos o negativos) con los valores predichos por nuestro modelo.\n",
    "\n",
    "La matriz consta de cuatro términos:\n",
    "- Verdaderos Positivos (VP): Casos en los que el modelo predijo correctamente la presencia de diabetes.\n",
    "- Falsos Positivos (FP): Casos en los que el modelo predijo incorrectamente la presencia de diabetes.\n",
    "- Verdaderos Negativos (VN): Casos en los que el modelo predijo correctamente la ausencia de diabetes.\n",
    "- Falsos Negativos (FN): Casos en los que el modelo predijo incorrectamente la ausencia de diabetes.\n",
    "\n",
    "## Exactitud y Precisión\n",
    "\n",
    "A partir de la matriz de confusión, podemos calcular dos métricas importantes:\n",
    "\n",
    "- **Exactitud (Accuracy)**: Mide la proporción de predicciones correctas (tanto VP como VN) en relación con el total de predicciones. Se calcula como: \n",
    "  $$\n",
    "  \\text{Exactitud} = \\frac{VP + VN}{VP + FP + FN + VN}\n",
    "  $$\n",
    "  La exactitud es útil cuando las clases están equilibradas, es decir, tienen aproximadamente la misma cantidad de observaciones.\n",
    "\n",
    "- **Precisión (Precision)**: Mide la proporción de predicciones positivas correctas en relación con el total de predicciones positivas. Se calcula como:\n",
    "  $$\n",
    "  \\text{Precisión} = \\frac{VP}{VP + FP}\n",
    "  $$\n",
    "  La precisión es importante en situaciones donde los Falsos Positivos son más preocupantes que los Falsos Negativos.\n",
    "\n",
    "Estas métricas nos ayudan a entender la calidad del modelo y a identificar áreas donde el modelo puede necesitar mejoras, especialmente en el contexto de diagnóstico médico donde tanto los Falsos Positivos como los Falsos Negativos pueden tener consecuencias significativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Hiperparámetros\n",
    "\n",
    "Después de evaluar nuestro modelo inicial de Random Forest, el siguiente paso es intentar mejorar su rendimiento ajustando los hiperparámetros. Para ello, podemos utilizar dos métodos populares: la Búsqueda en Rejilla (Grid Search) y la Búsqueda Aleatoria (Random Search).\n",
    "\n",
    "## Búsqueda en Rejilla (Grid Search)\n",
    "\n",
    "La Búsqueda en Rejilla consiste en definir un 'grid' o rejilla de hiperparámetros y probar todas las combinaciones posibles. Este método es exhaustivo y nos asegura que encontraremos la mejor combinación de parámetros dentro del rango definido. Sin embargo, puede ser computacionalmente costoso y lento, especialmente si el conjunto de datos es grande y hay muchos hiperparámetros para ajustar.\n",
    "\n",
    "## Búsqueda Aleatoria (Random Search)\n",
    "\n",
    "La Búsqueda Aleatoria, por otro lado, selecciona combinaciones aleatorias de hiperparámetros para probar. Esto reduce el número de combinaciones que necesitamos evaluar y puede ser mucho más rápido que la Búsqueda en Rejilla. Aunque no garantiza encontrar la mejor combinación posible de hiperparámetros, puede acercarse bastante en mucho menos tiempo.\n",
    "\n",
    "## ¿Cuál es mejor?\n",
    "\n",
    "La elección entre Búsqueda en Rejilla y Búsqueda Aleatoria depende del tamaño del conjunto de datos, la cantidad de hiperparámetros y los recursos computacionales disponibles. En general, la Búsqueda Aleatoria es una buena opción cuando se tiene un tiempo o recursos limitados, mientras que la Búsqueda en Rejilla es adecuada cuando se necesita la mejor combinación posible de hiperparámetros y se dispone de suficientes recursos para realizar el cálculo.\n",
    "\n",
    "En la siguiente sección, aplicaremos una de estas técnicas para mejorar nuestro modelo de Random Forest y compararemos los resultados con nuestro modelo inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Definir el conjunto de hiperparámetros para la búsqueda aleatoria\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 6, 8, 10, 12],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Realizar la búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   scoring='accuracy', n_iter=100, cv=3, verbose=2, \n",
    "                                   random_state=42, n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo a los datos\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print('Mejores parámetros:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest con los mejores hiperparámetros\n",
    "best_params = random_search.best_params_\n",
    "rf_optimized = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions_optimized = rf_optimized.predict(X_test)\n",
    "\n",
    "# Calcular la precisión y la exactitud\n",
    "accuracy_optimized = accuracy_score(y_test, predictions_optimized)\n",
    "precision_optimized = precision_score(y_test, predictions_optimized)\n",
    "\n",
    "# Calcular y visualizar la matriz de confusión\n",
    "conf_matrix_optimized = confusion_matrix(y_test, predictions_optimized)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix_optimized)\n",
    "\n",
    "# Imprimir las métricas de evaluación\n",
    "print(f'Precisión Optimizada: {precision_optimized}')\n",
    "print(f'Exactitud Optimizada: {accuracy_optimized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste Fino de Hiperparámetros con Búsqueda en Rejilla\n",
    "\n",
    "Después de realizar una búsqueda aleatoria para encontrar una buena combinación de hiperparámetros para nuestro modelo de Random Forest, el siguiente paso es realizar un ajuste más fino de estos parámetros. Para esto, utilizaremos la técnica de Búsqueda en Rejilla (Grid Search), que nos permite explorar en detalle un rango más estrecho de valores alrededor de los mejores parámetros encontrados previamente.\n",
    "\n",
    "## ¿Qué es el Ajuste Fino?\n",
    "\n",
    "El ajuste fino implica explorar de manera más exhaustiva y detallada los hiperparámetros de un modelo para encontrar la configuración óptima. A diferencia de la búsqueda aleatoria, que prueba una muestra aleatoria de combinaciones de hiperparámetros, la búsqueda en rejilla prueba todas las combinaciones posibles dentro de un rango específico definido por nosotros.\n",
    "\n",
    "## Implementación del Ajuste Fino\n",
    "\n",
    "Para implementar la búsqueda en rejilla, definimos un conjunto de valores para cada hiperparámetro que queremos ajustar. Estos valores estarán centrados alrededor de los mejores parámetros encontrados en la búsqueda aleatoria. Luego, utilizamos `GridSearchCV` de scikit-learn, que probará sistemáticamente todas las combinaciones de estos valores en un proceso de validación cruzada.\n",
    "\n",
    "## Beneficios del Ajuste Fino\n",
    "\n",
    "Este método nos permite:\n",
    "- Explorar en detalle un subconjunto específico de hiperparámetros.\n",
    "- Aumentar las posibilidades de encontrar la combinación más óptima dentro del rango especificado.\n",
    "- Mejorar potencialmente el rendimiento del modelo al ajustar con precisión los hiperparámetros.\n",
    "\n",
    "Al final de este proceso, tendremos una idea clara de cuáles son los mejores hiperparámetros dentro del rango que hemos definido, lo que nos ayudará a construir un modelo de Random Forest aún más efectivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejores parámetros:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el conjunto de hiperparámetros para la búsqueda en rejilla\n",
    "param_grid = {\n",
    "    'n_estimators': [395, 400, 405],  \n",
    "    'max_features': ['sqrt'],         \n",
    "    'max_depth': [3, 5, 10],         \n",
    "    'criterion': ['gini', 'entropy'],            \n",
    "    'min_samples_split': [5, 10],   \n",
    "    'min_samples_leaf': [1, 5]     \n",
    "}\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Realizar la búsqueda en rejilla\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Ajustar el modelo a los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print('Mejores parámetros:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest con los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "rf_optimized = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions_optimized = rf_optimized.predict(X_test)\n",
    "\n",
    "# Calcular la precisión y la exactitud\n",
    "accuracy_optimized = accuracy_score(y_test, predictions_optimized)\n",
    "precision_optimized = precision_score(y_test, predictions_optimized)\n",
    "\n",
    "# Calcular y visualizar la matriz de confusión\n",
    "conf_matrix_optimized = confusion_matrix(y_test, predictions_optimized)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix_optimized)\n",
    "\n",
    "# Imprimir las métricas de evaluación\n",
    "print(f'Precisión Optimizada: {precision_optimized}')\n",
    "print(f'Exactitud Optimizada: {accuracy_optimized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguientes Pasos Después de la Optimización de Hiperparámetros\n",
    "\n",
    "Aunque la optimización de hiperparámetros es una técnica poderosa para mejorar el rendimiento de los modelos de Machine Learning, a veces los incrementos en precisión o exactitud pueden ser modestos. Si tras ajustar los hiperparámetros del modelo de Random Forest sólo se observa una ligera mejora, como en nuestro caso, existen otras estrategias que podemos considerar:\n",
    "\n",
    "## 1. **Ingeniería de Características (Feature Engineering)**\n",
    "   La ingeniería de características puede tener un gran impacto en el rendimiento del modelo. Esto incluye:\n",
    "   - **Crear nuevas características** a partir de las existentes.\n",
    "   - **Transformar variables**: aplicar transformaciones logarítmicas, cuadráticas, etc., a las variables.\n",
    "   - **Codificación de variables categóricas**: si hay variables categóricas, considera diferentes técnicas de codificación.\n",
    "   - **Selección de características**: usar técnicas para seleccionar las características más importantes.\n",
    "\n",
    "## 2. **Manejo de Datos Desbalanceados**\n",
    "   Si el conjunto de datos está desbalanceado, es decir, una clase es mucho más frecuente que otra, puede afectar el rendimiento del modelo:\n",
    "   - Considera **técnicas de sobremuestreo** (como SMOTE) o **submuestreo** para equilibrar las clases.\n",
    "   - Utiliza parámetros como **class_weight** en modelos que lo soporten para dar más importancia a las clases menos frecuentes.\n",
    "\n",
    "## 3. **Probar Diferentes Modelos de Machine Learning**\n",
    "   A veces, un modelo diferente puede funcionar mejor para tu conjunto de datos específico:\n",
    "   - Prueba con diferentes algoritmos, como **SVM, KNN o Redes Neuronales**, y compara su rendimiento.\n",
    "   - Considera **modelos de ensamblaje** adicionales, como Gradient Boosting o XGBoost.\n",
    "\n",
    "## 4. **Validación Cruzada Avanzada**\n",
    "   En lugar de una simple división entrenamiento-prueba, usa **validación cruzada k-fold** para una mejor estimación del rendimiento del modelo.\n",
    "\n",
    "## 5. **Ajuste Fino de la Búsqueda de Hiperparámetros**\n",
    "   Si bien ya realizaste una búsqueda aleatoria, podrías:\n",
    "   - Realizar una **búsqueda en rejilla** centrada en torno a los mejores valores encontrados en la búsqueda aleatoria.\n",
    "   - Aumentar el número de iteraciones o explorar un rango más amplio de valores en la búsqueda aleatoria.\n",
    "\n",
    "## 6. **Análisis de Errores**\n",
    "   - Revisa los casos en los que el modelo se equivoca y trata de entender el por qué.\n",
    "   - Esto podría revelar pistas sobre qué ajustes se pueden hacer, ya sean en la pre-procesamiento de los datos o en ajustes del modelo.\n",
    "\n",
    "Recuerda que el proceso de modelado es iterativo y a menudo requiere experimentación y ajuste fino. No siempre hay una solución única, y lo que funciona para un conjunto de datos puede no funcionar para otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de la Importancia de las Variables\n",
    "\n",
    "En los modelos de Random Forest, es posible evaluar la importancia de cada variable (o característica) en la predicción del modelo. Esta importancia se basa en cuánto contribuye cada variable a la mejora de la pureza de los nodos en los árboles del bosque. Las variables que más contribuyen a la clasificación eficiente o a la reducción del error en la regresión son consideradas más importantes.\n",
    "\n",
    "## ¿Cómo se Mide la Importancia de las Variables?\n",
    "\n",
    "La importancia de las variables en Random Forest se mide a través de un valor conocido como \"feature importance\" o importancia de la característica. Este valor es un número que indica la importancia relativa de cada variable para el modelo. En términos técnicos, se basa en la disminución promedio de impureza (Gini impurity o MSE para regresión) que la variable aporta a los árboles del bosque.\n",
    "\n",
    "## Visualización de la Importancia de las Variables\n",
    "\n",
    "Una forma efectiva de visualizar la importancia de las variables es mediante un gráfico de barras que muestre la importancia de cada variable. Esto nos permite ver rápidamente qué variables tienen el mayor impacto en las predicciones del modelo.\n",
    "\n",
    "## Código para Evaluar la Importancia de las Variables\n",
    "\n",
    "El siguiente código muestra cómo obtener y visualizar la importancia de las variables en un modelo de Random Forest entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf_optimized.feature_importances_\n",
    "\n",
    "# Convertir las importancias en un DataFrame para facilitar la visualización\n",
    "feature_importances_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importances_df = feature_importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_importances_df['feature'], feature_importances_df['importance'])\n",
    "plt.xlabel('Características')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia de las Características en el Modelo Random Forest')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones con Random Forest\n",
    "\n",
    "Después de explorar el uso de Random Forest para problemas de clasificación, ahora nos centraremos en su aplicación en problemas de regresión. El algoritmo de Random Forest es versátil y puede ser utilizado tanto para clasificación como para regresión. Sin embargo, hay diferencias fundamentales entre estos dos tipos de problemas que afectan cómo se construye y evalúa el modelo.\n",
    "\n",
    "## Diferencia entre Clasificación y Regresión\n",
    "\n",
    "La principal diferencia entre clasificación y regresión radica en el tipo de variable objetivo que se está prediciendo:\n",
    "\n",
    "- **Clasificación**: En los problemas de clasificación, la variable objetivo es categórica. Los modelos de clasificación predicen etiquetas discretas (como 'sí' o 'no', 'spam' o 'no spam', etc.). En Random Forest, esto se logra mediante el uso de `RandomForestClassifier` en scikit-learn, donde el algoritmo realiza votaciones entre los árboles para decidir la clase de cada observación.\n",
    "\n",
    "- **Regresión**: En los problemas de regresión, la variable objetivo es continua. Los modelos de regresión predicen valores numéricos (como el precio de una casa, la temperatura, etc.). Para la regresión con Random Forest, se utiliza `RandomForestRegressor`. Aquí, el algoritmo promedia las predicciones de todos los árboles para obtener un valor continuo.\n",
    "\n",
    "## Implementación de Random Forest para Regresión\n",
    "\n",
    "Para implementar un modelo de regresión con Random Forest, utilizamos la clase `RandomForestRegressor` de scikit-learn. Este modelo se comporta de manera similar al clasificador, pero está optimizado para predecir valores numéricos continuos en lugar de etiquetas de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = diabetes_df.drop('Glucosa', axis=1)\n",
    "y = diabetes_df['Glucosa']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest para regresión\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "predictions = rf_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Error Cuadrático Medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de Determinación (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Rendimiento en Regresión: MSE y R²\n",
    "\n",
    "Al trabajar con modelos de regresión, como el Random Forest para regresión, es crucial evaluar su rendimiento utilizando métricas adecuadas. Dos de las métricas más comunes en regresión son el Error Cuadrático Medio (MSE) y el Coeficiente de Determinación (R²).\n",
    "\n",
    "## Error Cuadrático Medio (MSE)\n",
    "\n",
    "El MSE es una medida que calcula el promedio de los cuadrados de los errores, es decir, la diferencia cuadrada entre los valores observados y los predichos. Se define como:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $y_i$ es el valor observado.\n",
    "- $\\hat{y_i}$ es el valor predicho.\n",
    "- $n$ es el número de observaciones.\n",
    "\n",
    "Un MSE más bajo indica un mejor rendimiento, ya que significa que la diferencia entre los valores observados y predichos es, en promedio, menor. Sin embargo, es importante tener en cuenta que el MSE es sensible a los valores atípicos, ya que penaliza más los errores grandes.\n",
    "\n",
    "## Coeficiente de Determinación (R²)\n",
    "\n",
    "El R² es una medida que indica qué proporción de la varianza en la variable dependiente es predecible a partir de las variables independientes. Se calcula como:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\bar{y}$ es la media de los valores observados.\n",
    "\n",
    "El R² varía entre 0 y 1, donde un valor más cercano a 1 indica que una mayor proporción de la varianza está siendo explicada por el modelo, lo cual es deseable. Un R² bajo sugiere que el modelo no explica bien la varianza de los datos.\n",
    "\n",
    "## Interpretación\n",
    "\n",
    "- **MSE**: Proporciona una idea de cuán cerca están las predicciones del modelo a los valores reales en términos de error absoluto.\n",
    "- **R²**: Ofrece una visión de qué tan bien las predicciones del modelo se ajustan a la variabilidad de los datos.\n",
    "\n",
    "Ambas métricas son importantes para evaluar y comparar modelos de regresión, pero siempre deben ser consideradas en el contexto del problema específico y del conjunto de datos con los que se está trabajando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Hiperparámetros en Random Forest para Regresión\n",
    "\n",
    "Tras evaluar nuestro modelo inicial de regresión de Random Forest utilizando MSE y R², el próximo paso es optimizar los hiperparámetros para mejorar estas métricas. Al igual que en los problemas de clasificación, podemos utilizar técnicas como la Búsqueda Aleatoria (Random Search) y la Búsqueda en Rejilla (Grid Search) para encontrar la mejor combinación de hiperparámetros.\n",
    "\n",
    "## Enfoque para la Optimización\n",
    "\n",
    "El objetivo es ajustar los hiperparámetros de tal manera que reduzcamos el error de predicción (bajo MSE) y aumentemos la varianza explicada (alto R²). Las técnicas de búsqueda nos permiten explorar sistemáticamente una gama de posibles valores de hiperparámetros y evaluar cuál combinación ofrece el mejor rendimiento.\n",
    "\n",
    "## Parámetros Importantes en RandomForestRegressor\n",
    "\n",
    "Algunos de los hiperparámetros clave en `RandomForestRegressor` que generalmente se ajustan incluyen:\n",
    "\n",
    "- `n_estimators`: Número de árboles en el bosque.\n",
    "- `max_features`: Número máximo de características consideradas para dividir un nodo.\n",
    "- `max_depth`: Profundidad máxima de los árboles.\n",
    "- `min_samples_split`: Número mínimo de muestras requeridas para dividir un nodo interno.\n",
    "- `min_samples_leaf`: Número mínimo de muestras requeridas para estar en un nodo hoja.\n",
    "- `bootstrap`: Método para muestrear los datos (con o sin reemplazo).\n",
    "\n",
    "## Implementación de la Optimización\n",
    "\n",
    "La implementación de estas técnicas de búsqueda es similar a la que se utiliza en clasificación, ajustando los parámetros específicos para el contexto de la regresión. Después de encontrar la mejor combinación de parámetros, evaluamos el modelo nuevamente para ver si hay una mejora en MSE y R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificacion con las 5 variables más importantes identificadas arriba\n",
    "# Busqueda de hiperparámetros para el modelo de regresión\n",
    "# Usen el reporte de evaluacion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
